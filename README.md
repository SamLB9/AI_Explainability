# AI_Explainability

Artificial Intelligence (AI) makes it possible to harness huge volumes of data to perform complex tasks, usually very efficiently. However, AI models, and more specifically Machine Learning and even Deep Learning models, are often compared to "black boxes" due to their opaque operation, not necessarily understandable by humans. The complexity of these models is therefore a strength, as it enables us to respond to problems better than with simpler models, but it is also a weakness, as it makes them difficult to interpret. However, in certain critical fields such as medical diagnostics or autonomous driving, where human lives may be at stake, control and understanding of the decision-making mechanisms of these models is essential.

In concrete terms, explicability is a field of machine learning that aims to justify as precisely as possible a result given by a model. Explainability is developing remarkably well. Research in this field is very active, and companies are investing more and more in these issues.

AI explicability or interpretability aims to make the operation and results of models more intelligible and transparent to humans, without compromising on performance.
However, the difficulty today is that the more precise the models, the less explicable they become. So we're looking for a compromise. Even if for many data scientists, precision seems to be the supreme indicator of model quality, it's important to understand that it isn't everything. In deep learning, for example, explaining models is very difficult. Finding an algorithm as efficient as deep learning that is also 100% explainable is (for the moment?) utopian.

This need for explicability concerns different players, each with their own motivations. Firstly, for the Data Scientist who develops the model, a better understanding can help correct certain problems and improve the model. On the business side, explainability can be a means of assessing how well the model matches the company's strategy and objectives. Explanability can also be used to test the model's robustness, reliability and potential impact on customers. Finally, it can enable the customer who would be the object of a decision by a system based on an AI model to be informed of the impact of this decision and the potential actions possible to modify it.

![image](https://github.com/SamLB9/AI_Explainability/assets/106078401/08a09646-078c-4e6b-b175-3af1c75b029f)

In certain fields, such as health or law, the use of machine learning models that cannot be explained raises ethical questions and can prove dangerous. This aspect should be much more closely regulated. It could be interesting to create a text similar to the RGPD, but for explicability, which is an issue at least as sensitive as user data protection.

## How to explain a machine learning model and how it works?

### 1) LIME (Local Interpretable Model-agnostic Explanations):

One of the most popular explicability methods is LIME (Local Interpretable Model-agnostic Explanations). Its principle is simple: locally approximate the complex model with a simpler, and therefore interpretable, model. This method makes it possible to explain the model's decision concerning a particular observation. New instances close to the latter are generated by perturbing the values of the variables. These new instances are weighted according to their proximity to the instance to be explained. Predictions are then made for these new instances. A simple model, such as a linear regression, is finally fitted to these new instances and the associated predictions to produce the explanation.

Example of a local explanation generated by the LIME method:

![image](https://github.com/SamLB9/AI_Explainability/assets/106078401/2ce4617f-8b1e-4931-9147-d1a20730348d)

For the chosen customer, the explanation provided by the LIME method indicates that the probability of cancellation is 83%, and that this prediction is positively influenced by the fact that this customer has taken out a monthly contract with optical fiber.

### 2) Ancres:

The Anchors method has been developed to overcome some of LIME's problems, notably concerning the generalizability of explanations, which is not clearly defined. The principle of this second method is to define decision rules that anchor a prediction, using an optimized search algorithm.

Example of a local explanation generated by the ancres method:

![image](https://github.com/SamLB9/AI_Explainability/assets/106078401/62ac21db-a884-48ee-a69e-b55c9fe9a5c8)

The explanation given for the chosen customer indicates the variables that anchor the churn prediction: a rather low seniority (less than 29 months), a type of electronic payment, a monthly contract, etc. The added value of this method is linked to an additional piece of information, the coverage. In this case, 8% of customers have the characteristics defined by the anchor.

### 3) Counterfactual explanations:

In contrast to the previous method, which focuses on the variables that anchor predictions, the counterfactual explanation method looks for those whose change modifies the prediction. Several approaches exist to find the smallest possible change that modifies the prediction, including the naive trial-and-error approach or the use of an optimization algorithm. The advantage of this method is that it generates several explanations for a single decision.

Example of a counterfactual explanation:

![image](https://github.com/SamLB9/AI_Explainability/assets/106078401/9f34fe28-4f5a-49e3-b150-e9b75c86ae1a)

Still on the subject of the chosen customer, a first counterfactual explanation consists in the modification of his type of contract and his type of payment; with these two changes, the model predicts non-cancellation. The same applies if only the customer's seniority is modified, from 13 to 65 months.

### In conclusion, these three methods produce different explanations, but they seem to be consistent with each other, since certain variables are found in all the explanations. This is the case for contract type, which appears to be an important factor in prediction. This seems relevant, since it is generally easier to cancel a monthly contract, often without commitment. Finally, explicability methods vary in terms of the techniques used, visualization and type of explanation. One of the difficulties therefore lies in evaluating these methods. However, it is possible to define criteria for comparing them, and to look at their respective advantages and disadvantages. These criteria can be used to select the right method(s) for a given use case.





